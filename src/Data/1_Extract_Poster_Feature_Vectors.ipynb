{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Poster Feature Vectors\n",
    "\n",
    "This notebook details the steps needed to extract pretrained ResNet50 feature vectors for movie poster thumbnails stored in the '.../thumbnails' directory. This directory does not contain any other folders other than the thumbnails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import movieids from thumbnail folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt1467265.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt6396074.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt2013243.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0119103.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt7218564.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         movieid\n",
       "0  tt1467265.jpg\n",
       "1  tt6396074.jpg\n",
       "2  tt2013243.jpg\n",
       "3  tt0119103.jpg\n",
       "4  tt7218564.jpg"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "df_posters = pd.DataFrame({\"movieid\":os.listdir(\"../data/posters/thumbnails\")})\n",
    "df_posters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downselect to movieids with valid poster images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt1062961.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt3885736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0027902.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt5066056.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0052306.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         movieid\n",
       "0  tt1062961.jpg\n",
       "1  tt3885736.jpg\n",
       "2  tt0027902.jpg\n",
       "3  tt5066056.jpg\n",
       "4  tt0052306.jpg"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_posters = pd.read_csv('../data/posters/ValidPosters.csv')\n",
    "df_valid_posters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt1062961.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt3885736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0027902.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt5066056.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0052306.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         movieid\n",
       "0  tt1062961.jpg\n",
       "1  tt3885736.jpg\n",
       "2  tt0027902.jpg\n",
       "3  tt5066056.jpg\n",
       "4  tt0052306.jpg"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_process = df_valid_posters.merge(df_posters,how='left')\n",
    "df_process.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process.to_csv('Posters_movie_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieid</th>\n",
       "      <th>file_loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt1062961.jpg</td>\n",
       "      <td>/Users/krsrik/Documents/Projects/W266/w266-Kar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt3885736.jpg</td>\n",
       "      <td>/Users/krsrik/Documents/Projects/W266/w266-Kar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0027902.jpg</td>\n",
       "      <td>/Users/krsrik/Documents/Projects/W266/w266-Kar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt5066056.jpg</td>\n",
       "      <td>/Users/krsrik/Documents/Projects/W266/w266-Kar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0052306.jpg</td>\n",
       "      <td>/Users/krsrik/Documents/Projects/W266/w266-Kar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         movieid                                           file_loc\n",
       "0  tt1062961.jpg  /Users/krsrik/Documents/Projects/W266/w266-Kar...\n",
       "1  tt3885736.jpg  /Users/krsrik/Documents/Projects/W266/w266-Kar...\n",
       "2  tt0027902.jpg  /Users/krsrik/Documents/Projects/W266/w266-Kar...\n",
       "3  tt5066056.jpg  /Users/krsrik/Documents/Projects/W266/w266-Kar...\n",
       "4  tt0052306.jpg  /Users/krsrik/Documents/Projects/W266/w266-Kar..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_process.loc[:,'file_loc'] = '../data/posters/thumbnails/' + df_process.movieid\n",
    "df_process.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup pretrained ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "keras_layer (KerasLayer)     (None, 2048)              23564800  \n",
      "=================================================================\n",
      "Total params: 23,564,800\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "resnet_layer = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\",trainable=False)\n",
    "\n",
    "feature_vector = resnet_layer(inputs)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=feature_vector)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize and Normalize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # Specify height and width of image to match the input format of the model\n",
    "CHANNELS = 3 # Keep RGB color channels to match the input format of the model\n",
    "def parse_function(filename,label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read an image from a file\n",
    "        image_string = tf.io.read_file(filename)\n",
    "        # Decode it into a dense vector\n",
    "        image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "        # Resize it to fixed shape\n",
    "        image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "        # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "        image_normalized = image_resized / 255.0\n",
    "    except:\n",
    "        image_normalized = None\n",
    "    return image_normalized, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Images for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024 # Big enough to measure an F1-score\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if is_training == True:\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        dataset = dataset.cache()\n",
    "        # Shuffle the data each buffer size\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "        \n",
    "    # Batch the data for multiple steps\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list(df_process.file_loc)\n",
    "labels = list(df_process.movieid)\n",
    "dataset = create_dataset(filenames, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Feature Vectors for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "Batch #1 completed in 99.71660876274109 seconds...\n",
      "Batch #2 completed in 93.30944585800171 seconds...\n",
      "Batch #3 completed in 93.17532110214233 seconds...\n",
      "Batch #4 completed in 95.90331816673279 seconds...\n",
      "Batch #5 completed in 87.30925512313843 seconds...\n",
      "Batch #6 completed in 98.59444499015808 seconds...\n",
      "Batch #7 completed in 107.37575387954712 seconds...\n",
      "Batch #8 completed in 99.47844314575195 seconds...\n",
      "Batch #9 completed in 97.47757983207703 seconds...\n",
      "Batch #10 completed in 105.36129307746887 seconds...\n",
      "Batch #11 completed in 110.07495784759521 seconds...\n",
      "Batch #12 completed in 107.26291298866272 seconds...\n",
      "Batch #13 completed in 99.03047275543213 seconds...\n",
      "Batch #14 completed in 112.88965392112732 seconds...\n",
      "Batch #15 completed in 122.93072271347046 seconds...\n",
      "Batch #16 completed in 104.8819739818573 seconds...\n",
      "Batch #17 completed in 100.13288593292236 seconds...\n",
      "Batch #18 completed in 109.38337516784668 seconds...\n",
      "Batch #19 completed in 121.76791787147522 seconds...\n",
      "Batch #20 completed in 107.23518705368042 seconds...\n",
      "Batch #21 completed in 105.42177772521973 seconds...\n",
      "Batch #22 completed in 114.82782101631165 seconds...\n",
      "Batch #23 completed in 108.27985501289368 seconds...\n",
      "Batch #24 completed in 133.0462567806244 seconds...\n",
      "Batch #25 completed in 111.90745496749878 seconds...\n",
      "Batch #26 completed in 145.448575258255 seconds...\n",
      "Batch #27 completed in 121.89525198936462 seconds...\n",
      "Batch #28 completed in 113.76688814163208 seconds...\n",
      "Batch #29 completed in 111.50643301010132 seconds...\n",
      "Batch #30 completed in 123.61066699028015 seconds...\n",
      "Batch #31 completed in 128.27373480796814 seconds...\n",
      "Batch #32 completed in 119.89927816390991 seconds...\n",
      "Batch #33 completed in 132.5023317337036 seconds...\n",
      "Batch #34 completed in 124.56544613838196 seconds...\n",
      "Batch #35 completed in 128.1276319026947 seconds...\n",
      "Batch #36 completed in 114.87672805786133 seconds...\n",
      "Batch #37 completed in 120.65604186058044 seconds...\n",
      "Batch #38 completed in 121.6051881313324 seconds...\n",
      "Batch #39 completed in 116.3924868106842 seconds...\n",
      "Batch #40 completed in 105.32085585594177 seconds...\n",
      "Batch #41 completed in 111.3251781463623 seconds...\n",
      "Batch #42 completed in 104.86701607704163 seconds...\n",
      "Batch #43 completed in 111.35660696029663 seconds...\n",
      "Batch #44 completed in 103.13080596923828 seconds...\n",
      "Batch #45 completed in 104.93728017807007 seconds...\n",
      "Batch #46 completed in 113.02076697349548 seconds...\n",
      "Batch #47 completed in 136.73029494285583 seconds...\n",
      "Batch #48 completed in 123.60525798797607 seconds...\n",
      "Batch #49 completed in 120.68715000152588 seconds...\n",
      "Batch #50 completed in 111.87920784950256 seconds...\n",
      "Batch #51 completed in 113.88129115104675 seconds...\n",
      "Batch #52 completed in 115.51198434829712 seconds...\n",
      "Batch #53 completed in 93.9060869216919 seconds...\n",
      "Batch #54 completed in 113.78434777259827 seconds...\n",
      "Batch #55 completed in 108.59878420829773 seconds...\n",
      "Batch #56 completed in 101.26401829719543 seconds...\n",
      "Batch #57 completed in 95.70139503479004 seconds...\n",
      "Batch #58 completed in 111.88624119758606 seconds...\n",
      "Batch #59 completed in 100.97447085380554 seconds...\n",
      "Batch #60 completed in 92.51900601387024 seconds...\n",
      "Batch #61 completed in 90.65181183815002 seconds...\n",
      "Batch #62 completed in 87.32352423667908 seconds...\n",
      "Batch #63 completed in 96.85966491699219 seconds...\n",
      "Batch #64 completed in 103.19930005073547 seconds...\n",
      "Batch #65 completed in 85.09891390800476 seconds...\n",
      "Batch #66 completed in 90.95389008522034 seconds...\n",
      "Batch #67 completed in 91.37888789176941 seconds...\n",
      "Batch #68 completed in 86.84396696090698 seconds...\n",
      "Batch #69 completed in 91.41672897338867 seconds...\n",
      "Batch #70 completed in 98.11511301994324 seconds...\n",
      "Batch #71 completed in 84.45751595497131 seconds...\n",
      "Batch #72 completed in 83.85866117477417 seconds...\n",
      "Batch #73 completed in 88.44642114639282 seconds...\n",
      "Batch #74 completed in 90.92056703567505 seconds...\n",
      "Batch #75 completed in 85.55061793327332 seconds...\n",
      "Batch #76 completed in 84.57133793830872 seconds...\n",
      "Batch #77 completed in 77.82196617126465 seconds...\n",
      "Batch #78 completed in 73.8044171333313 seconds...\n",
      "Batch #79 completed in 73.90167808532715 seconds...\n",
      "Batch #80 completed in 16.824571132659912 seconds...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "NUM_BATCHES = int(np.ceil(df_process.shape[0]/BATCH_SIZE))\n",
    "print(NUM_BATCHES)\n",
    "f_vectors = np.array([])\n",
    "label_vector = []\n",
    "ctr = 1\n",
    "for f, l in dataset.take(NUM_BATCHES):\n",
    "    start_time = time.time()\n",
    "    if ctr ==1:\n",
    "        f_vectors = model(f).numpy()       \n",
    "    else:\n",
    "        f_vectors = np.append(f_vectors,model(f).numpy(),axis=0)\n",
    "    label_vector.extend(l.numpy())\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(\"Batch #\" + str(ctr) + \" completed in \" + str(duration) + \" seconds...\")\n",
    "    ctr +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save feature vectors to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81124, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(f_vectors.shape)\n",
    "np.save('../data/image_feature_vectors.data', f_vectors)\n",
    "df_process.movieid.to_csv('../data/image_tracking_id.data', header=None, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
