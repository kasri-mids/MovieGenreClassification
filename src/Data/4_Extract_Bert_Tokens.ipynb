{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. Extract Bert Tokens.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUybYgwnEFUrOJlVi4aT88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasri-mids/MovieGenreClassification/blob/master/src/Data/4_Extract_Bert_Tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlbZwXllLMNS",
        "colab_type": "text"
      },
      "source": [
        "# Extracting BERT Tokens for train, dev and val\n",
        "\n",
        "In this notebook, we take our cleaned dataset and generate BERT tokens from the movie plot summaries which are then split into a train, dev and validation set and we save those files for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmbSAkVQvzG9",
        "colab_type": "code",
        "outputId": "f4037ef4-8da1-4d88-c664-215da8c40fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 45.3MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.27.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.34.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.18.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=b8d19f314717ba6247ac7b21de871823c47a388b4646b034599bb7c484974ff7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq9QHyqDv5go",
        "colab_type": "code",
        "outputId": "ac9b4780-555c-423d-db17-4b10b43f224d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16x7ej_5v6jR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from time import time\n",
        "import io\n",
        "import re\n",
        "from csv import reader\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.keras.layers import Lambda\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.layers import Dense, TimeDistributed\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah3wm1R5v7fY",
        "colab_type": "code",
        "outputId": "ffd08bc4-68f6-46ba-eb71-c9ab463463a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z001k5uIv8yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installing relevant modules\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za1m7JR8v-t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define maximal length of input 'sentences' (post tokenization).\n",
        "max_length = 128\n",
        "\n",
        "# ERROR TEST\n",
        "ERROR_TEST = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8lJphXVwAKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here, only have to run once because we are tokenizing the movieplot which is same regardless of NUM_LABELS\n",
        "if (ERROR_TEST):\n",
        "    NUM_LABELS = 1\n",
        "    \n",
        "    df_train = pd.read_csv('/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/clean_df_train_labels_' + str(NUM_LABELS) + '.csv')\n",
        "    df_dev = pd.read_csv('/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/clean_df_dev_labels_' + str(NUM_LABELS) + '.csv')\n",
        "    df_val = pd.read_csv('/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/clean_df_val_labels_' + str(NUM_LABELS) + '.csv')\n",
        "    \n",
        "    data_save_prefix = '/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/baseline_test/'\n",
        "    model_save_prefix = '/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/model/baseline_test/'\n",
        "    plot_save_prefix = '/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/plots/baseline_test/'\n",
        "\n",
        "else:\n",
        "    NUM_LABELS = 10\n",
        "    \n",
        "    df_train = pd.read_csv('/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/clean_df_train_labels_' + str(NUM_LABELS) + '.csv')\n",
        "    df_dev = pd.read_csv('/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/clean_df_dev_labels_' + str(NUM_LABELS) + '.csv')\n",
        "    df_val = pd.read_csv('/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/clean_df_val_labels_' + str(NUM_LABELS) + '.csv')\n",
        "\n",
        "    data_save_prefix = '/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/baseline/'\n",
        "    model_save_prefix = '/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/model/baseline/'\n",
        "    plot_save_prefix = '/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/plots/baseline/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUgxlQgPwBvG",
        "colab_type": "code",
        "outputId": "a219303e-405d-4110-bb51-d099cc653be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieid_poster</th>\n",
              "      <th>indexes</th>\n",
              "      <th>movieid</th>\n",
              "      <th>movieplot</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tt1786530.jpg</td>\n",
              "      <td>73777</td>\n",
              "      <td>tt1786530.jpg</td>\n",
              "      <td>When a mysterious but attractive stranger appl...</td>\n",
              "      <td>[0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tt0146592.jpg</td>\n",
              "      <td>10645</td>\n",
              "      <td>tt0146592.jpg</td>\n",
              "      <td>A lonely obese nurse, working at a hospital te...</td>\n",
              "      <td>[1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tt0088153.jpg</td>\n",
              "      <td>2041</td>\n",
              "      <td>tt0088153.jpg</td>\n",
              "      <td>Doc Jenkins is a singer/songwriter who tries t...</td>\n",
              "      <td>[1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tt3733098.jpg</td>\n",
              "      <td>79291</td>\n",
              "      <td>tt3733098.jpg</td>\n",
              "      <td>Rosa is a mature police officer with both a ga...</td>\n",
              "      <td>[1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tt1657507.jpg</td>\n",
              "      <td>59996</td>\n",
              "      <td>tt1657507.jpg</td>\n",
              "      <td>In early-1990s Bogotá--after witnessing the gr...</td>\n",
              "      <td>[1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  movieid_poster  ...  labels\n",
              "0  tt1786530.jpg  ...     [0]\n",
              "1  tt0146592.jpg  ...     [1]\n",
              "2  tt0088153.jpg  ...     [1]\n",
              "3  tt3733098.jpg  ...     [1]\n",
              "4  tt1657507.jpg  ...     [1]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFNo5l5TwEIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "\n",
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_layer = hub.KerasLayer(BERT_MODEL_HUB)\n",
        "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "    tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = create_tokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31NQNE_-xRkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addWord(word):\n",
        "    \"\"\"\n",
        "    Convert a word into a word token. Note that the word can be tokenized to two or more tokens. \n",
        "        \n",
        "    arguments: word, pos label, ner label\n",
        "    returns: dictionary with tokens and labels\n",
        "    \"\"\"\n",
        "    # the dataset contains various '\"\"\"' combinations which we choose to truncate to '\"', etc. \n",
        "    if word == '\"\"\"\"':\n",
        "        word = '\"'\n",
        "    elif word == '``':\n",
        "        word = '`'\n",
        "        \n",
        "    tokens = tokenizer.tokenize(word)\n",
        "    tokenLength = len(tokens) # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
        "    \n",
        "    addDict = dict()\n",
        "    \n",
        "    addDict['wordToken'] = tokens\n",
        "    addDict['tokenLength'] = tokenLength\n",
        "    \n",
        "    return addDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDDf0CZhxTYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_bert_sentences(df, plotCol, max_length = 128):\n",
        "    \"\"\"\n",
        "        Read the file line by line and construct sentences. A sentence end is marked by the word 'sentence' in the next row.\n",
        "        You need to take care of that. Also, you need to cap sentence length using max_length. Sentences which are shorter than \n",
        "        max_length need to be padded. Also, we choose to end all sentences with a [SEP] token, padded or not. \n",
        "    \"\"\"\n",
        "\n",
        "    # lists for sentences, tokens, labels, etc.  \n",
        "    sentenceList = []\n",
        "    sentenceTokenList = []\n",
        "    sentLengthList = []\n",
        "\n",
        "    # lists for BERT input\n",
        "    bertSentenceIDs = []\n",
        "    bertMasks = []\n",
        "    bertSequenceIDs = []\n",
        "    # genreLabels = []\n",
        "    ctr = 0\n",
        "\n",
        "    for index,rows in df.iterrows():\n",
        "        ctr += 1\n",
        "\n",
        "        sentence = ''\n",
        "        sum_plot = rows[plotCol]\n",
        "        # genreLabels.append((rows['labels']))\n",
        "        sent = re.sub(r'(?!(([^\"]*\"){2})*[^\"]*$),', '', sum_plot)  # deal with '\"10,000\"' and convert them to '10000' -> TO DO : Is this working?\n",
        "        words = sent.split()\n",
        "        sentenceTokens = ['[CLS]']\n",
        "        sentenceLength = len(words)\n",
        "        for word in words:\n",
        "            sentence += ' ' + word\n",
        "            addDict = addWord(word)\n",
        "            sentenceTokens += addDict['wordToken']\n",
        "            # Create space for at least a final '[SEP]' token\n",
        "\n",
        "        sentenceLength = len(sentenceTokens)\n",
        "        if sentenceLength >= max_length - 1: \n",
        "            sentenceTokens = sentenceTokens[:max_length - 2]\n",
        "\n",
        "        sentenceLength = min(max_length -1, len(sentenceTokens))\n",
        "        sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
        "\n",
        "        sentenceList.append(sentence)\n",
        "        sentenceTokenList.append(sentenceTokens)\n",
        "        sentLengthList.append(sentenceLength)\n",
        "        bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
        "        bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
        "        bertSequenceIDs.append([0] * (max_length))\n",
        "\n",
        "    return (bertSentenceIDs, bertMasks, bertSequenceIDs, sentenceTokenList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMd98-6FxVyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainBertSentenceIDs, trainBertMasks, trainBertSequenceIDs, trainSentenceTokenList = construct_bert_sentences(df_train, 'movieplot', max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VqiY_0QxasM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "devBertSentenceIDs, devBertMasks, devBertSequenceIDs, devSentenceTokenList = construct_bert_sentences(df_dev, 'movieplot', max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqwarZFUxa2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valBertSentenceIDs, valBertMasks, valBertSequenceIDs, valSentenceTokenList = construct_bert_sentences(df_val, 'movieplot', max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSzDWpUixntK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array([(trainBertSentenceIDs),(trainBertMasks),(trainBertSequenceIDs)])\n",
        "X_dev = np.array([(devBertSentenceIDs),(devBertMasks),(devBertSequenceIDs)])\n",
        "X_val = np.array([(valBertSentenceIDs),(valBertMasks),(valBertSequenceIDs)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzq8dbD0x9Tw",
        "colab_type": "code",
        "outputId": "e4ad0f6d-fe6a-4039-8fbc-ab434efa2177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_dev.shape)\n",
        "print(X_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 46970, 128)\n",
            "(3, 15438, 128)\n",
            "(3, 15600, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9iJzFKIyFVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Always the same, regardless of the number of labels\n",
        "np.save('/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/clean_train_bert_tokens_' + str(max_length) + '.data', X_train)\n",
        "np.save('/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/clean_dev_bert_tokens_' + str(max_length) + '.data', X_dev)\n",
        "np.save('/content/drive/My Drive/w266-FinalProject/MoviePlotsAndPosters/data/clean_val_bert_tokens_' + str(max_length) + '.data', X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}